{
 "metadata": {
  "name": "",
  "signature": "sha256:38909e931009dbb84f5bceddf1037e947eb8c35a91509ce51e042c064f30b6ed"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import word_tokenize\n",
      "import string\n",
      "import os\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from collections import Counter\n",
      "from random import randint\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.metrics import accuracy_score, f1_score\n",
      "from sklearn.decomposition import PCA\n",
      "from matplotlib.colors import ColorConverter\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "from sklearn.manifold import MDS\n",
      "import matplotlib.patches as mpatches\n",
      "\n",
      "ig_path = \"/Users/edwinroland/Desktop/Iterating Grace/Iterating Grace.txt\"\n",
      "suspect_path = \"/Users/edwinroland/Desktop/Iterating Grace/suspects/\"\n",
      "\n",
      "# The paths in this script are set up assuming that there is a folder containing\n",
      "# .txt files for only the suspects' texts, and that there is a separate location\n",
      "# for \"Iterating Grace.txt\". The names of suspect's .txt files will be used to\n",
      "# identify them in the pandas dataframe and visualizations.\n",
      "\n",
      "\n",
      "# pronoun list to remove from text\n",
      "engl_pron = ['i', 'me','my', 'mine','myself','you', 'your','yours','yourself',\\\n",
      "            'she', 'her', 'hers', 'herself', 'he', 'him', 'his', 'himself',\\\n",
      "            'it', 'its', 'itself', 'we','us','our', 'ours','ourselves',\\\n",
      "            'yourselves', 'they','them','their', 'theirs', 'themselves',\\\n",
      "            'im', 'youre', 'youll','youd', 'shes', 'hes', 'hed', 'theyre', 'theyd']\n",
      "\n",
      "# takes list of tokens, returns string of randomly selected tokens separated by space\n",
      "# (sklearn's \"Vectorizer\" likes to take a string as its input)\n",
      "def random_bag(tokens, bag_size = 1971): # under this method, there are 1971 tokens in IG\n",
      "    auth_bag = []\n",
      "    counter = []\n",
      "    while len(counter)<bag_size:\n",
      "        j = randint(0,len(tokens)-1)\n",
      "        if j not in counter:\n",
      "            auth_bag.append(tokens[j])\n",
      "            counter.append(j)\n",
      "    return \" \".join(auth_bag)\n",
      "\n",
      "# same as random_bag, but returns continuous passages of text\n",
      "def passage_bag(tokens, bag_size = 1971): # under this method, there are 1971 tokens in IG\n",
      "    j = randint(0,len(tokens)-bag_size)\n",
      "    auth_bag = tokens[j:j+bag_size]\n",
      "    return \" \".join(auth_bag)\n",
      "\n",
      "\n",
      "# finds interquartile range of a list of numbers; handy for stats\n",
      "def iqr(dist):\n",
      "    q75, q25 = np.percentile(dist, [75 ,25])\n",
      "    return q75 - q25"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## IMPORT TEXTS\n",
      "\n",
      "# text of \"IG\"\n",
      "with open(ig_path) as f:\n",
      "    iterating_string = f.read()\n",
      "\n",
      "# text of \"IG\" tokenized (quick and dirty)\n",
      "# Note: the first step is stripping non-ascii characters, bc python 2.X doesn't like unicode\n",
      "ig_tokens = [x for x in word_tokenize(\"\".join([x for x in iterating_string if ord(x)<128]))\\\n",
      "             if x not in string.punctuation]\n",
      "\n",
      "# import each suspect's text into a dataframe, tokenizes text as well\n",
      "\n",
      "auth_frame = pd.DataFrame()\n",
      "auth_name = []\n",
      "auth_text = []\n",
      "for x in [x for x in os.listdir(suspect_path) if x[-4:]=='.txt']:\n",
      "    auth_name.append(x[:-4])\n",
      "    with open(suspect_path+x) as f:\n",
      "        auth_text.append(f.read())\n",
      "auth_frame['AUTH_NAME'] = auth_name # This column is really just the filenames\n",
      "auth_frame['TEXT'] = auth_text\n",
      "auth_frame['TOKENS'] = [[x for x in word_tokenize(\"\".join([x for x in auth_text[i] if ord(x)<128]))\\\n",
      "                         if x not in string.punctuation] for i in range(len(auth_text))]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## DATA SAMPLING\n",
      "\n",
      "# Slices texts into n random bags, returns feature arrays\n",
      "# takes a while to run on my computer\n",
      "\n",
      "n = 100 # number of slices to take from each text\n",
      "bag_list = [] # each list item will be a string of randomly chosen words from suspect's writing\n",
      "labels = [] # keeps track of which string belongs to which author\n",
      "for i in range(len(auth_frame['TOKENS'])):\n",
      "    for j in range(n):\n",
      "        bag_list.append(random_bag(auth_frame['TOKENS'][i]))\n",
      "        #bag_list.append(passage_bag(auth_frame['TOKENS'][i])) # alternate function calls up passages\n",
      "        labels.append(i)\n",
      "\n",
      "\n",
      "\n",
      "## CLASSIFIER PIPELINE\n",
      "        \n",
      "# Vectorizer will take the list of strings and return a sparse matrix with the feature set        \n",
      "\n",
      "# CountVectorizer is where account for Eder's and Hoover's observations\n",
      "# remove pronouns; rule of thumb # of fts 500; min_df suggested at 0.6-0.8 (for large # suspects)\n",
      "v = CountVectorizer(stop_words=engl_pron, max_features=500) #min_df=0.6\n",
      "bag_mtrx = v.fit_transform(bag_list)\n",
      "grc_mtrx = v.transform([random_bag(ig_tokens)])\n",
      "# grc_mtrx = v.transform([random_bag(ig_tokens)]) # alternate function calls up passages\n",
      "\n",
      "# many functions prefer arrays to sparse matrices, and with # of suspects < 10 and n (slices) < 1000,\n",
      "# we don't have so much data that an array is unwieldy\n",
      "bag_array = bag_mtrx.toarray()\n",
      "grc_array = grc_mtrx.toarray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## AUTHOR PREDICTION\n",
      "\n",
      "# SVM classifier (per Eder's empirical findings, convenience in sklearn)\n",
      "# reports overall accuracy on cross-validation, predicted author(s) of IG\n",
      "\n",
      "clf = LinearSVC()\n",
      "\n",
      "# I personally like to run a classifier multiple times and check the median score (and IQR),\n",
      "# to get a sense of its robustness, so I have lists that collect each iteration's output\n",
      "\n",
      "accr = [] # overall accuracy of classification\n",
      "f1s = [] # F1 scores of classification\n",
      "auth_preds = [] # each iteration's prediction for IG's authorship\n",
      "\n",
      "for i in range(100):\n",
      "    train_features, test_features, train_labels, test_labels = train_test_split(bag_array,\n",
      "                                                                                labels, test_size=0.25)\n",
      "    clf.fit(train_features, train_labels)\n",
      "    predicted = clf.predict(test_features)\n",
      "    \n",
      "    accr.append(accuracy_score(test_labels, predicted))\n",
      "    f1s.append(f1_score(test_labels, predicted))\n",
      "    auth_preds.append(clf.predict(grc_array)[0])\n",
      "\n",
      "print \"Median(IQR)   Accuracy:\",np.median(accr),\"(\", iqr(accr), \")\", \"  F1:\", np.median(f1s), '(', iqr(f1s), ')'\n",
      "print\n",
      "print \"Suspect List\"\n",
      "c = Counter(auth_preds)\n",
      "for k in c.keys():\n",
      "    # author name and its share of all predictions\n",
      "    print auth_frame[\"AUTH_NAME\"][k], int(100*float(c[k])/sum(c.values())), \"%\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## PCA GRAPH\n",
      "# all of this code replicates biplot() in R\n",
      "\n",
      "# list of colors in MPL\n",
      "colors = ColorConverter.cache.values()\n",
      "\n",
      "# PCA, naturally\n",
      "pca = PCA(n_components=4)\n",
      "\n",
      "## project feature arrays into PC space\n",
      "bag_pca = pca.fit_transform(bag_array)\n",
      "grc_pca = pca.transform(grc_array)\n",
      "\n",
      "# text information\n",
      "# 0,1 denote PC1 and PC2; change values for other PCs\n",
      "xs = bag_pca[:,0] # see 'prcomp(my_data)$x' in R\n",
      "ys = bag_pca[:,1]\n",
      "\n",
      "# loading information\n",
      "xvector = pca.components_[0] # see 'prcomp(my_data)$rotation' in R\n",
      "yvector = pca.components_[1]\n",
      "\n",
      "dvec = np.array([np.sqrt(xvector[i]**2+yvector[i]**2) for i in range(len(xvector))])\n",
      "load_ind = dvec.argsort()[-10:][::-1]\n",
      "\n",
      "plt.figure(1, figsize=(10, 10), dpi=200)    \n",
      "\n",
      "# plots suspects' texts\n",
      "for i in range(len(xs)):\n",
      "    plt.plot(xs[i], ys[i], 'x', color = colors[labels[i]+12])\n",
      "    \n",
      "# plots loadings\n",
      "for i in range(len(load_ind)):\n",
      "    plt.arrow(0, 0, xvector[load_ind[i]]*max(xs), yvector[load_ind[i]]*max(ys),\n",
      "              color='r', width=0.0005, head_width=0.0025)\n",
      "    plt.text(xvector[load_ind[i]]*max(xs)*1.2, yvector[load_ind[i]]*max(ys)*1.2,\n",
      "             list(v.get_feature_names())[load_ind[i]], color='r')\n",
      "\n",
      "# This is inelegant, but I was having trouble making a pretty legend\n",
      "import matplotlib.patches as mpatches\n",
      "\n",
      "#grn_patch = mpatches.Patch(color='r', label='\"Grace\"')\n",
      "grn_patch = mpatches.Patch(color=colors[len(auth_name)+12], label='\"Grace\"')\n",
      "h_list = [mpatches.Patch(color=colors[i+12], label = auth_frame['AUTH_NAME'][i]) for i in range(len(auth_name))]\n",
      "h_list.append(grn_patch)\n",
      "plt.legend(handles=h_list,loc=0)\n",
      "\n",
      "# plots IG\n",
      "plt.plot(grc_pca[0][0], grc_pca[0][1],  'o', color=colors[len(auth_name)+12])\n",
      "\n",
      "print 'Explained Variance', pca.explained_variance_ratio_[:2]\n",
      "# Haven't yet put this info on the axes like biplot() does!\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## COSINE SIMILARITY/MDS GRAPH\n",
      "# biplot modded for MDS with Cosine Similarity\n",
      "\n",
      "# MDS, naturally\n",
      "mds = MDS(n_components=2)\n",
      "new_array = np.concatenate((bag_array,grc_array), axis=0) # incl. IG vector with suspects'\n",
      "distances = 1 - cosine_similarity(new_array)\n",
      "\n",
      "# Scales Cosine Distances into 2-D space\n",
      "new_mds = mds.fit_transform(new_array)\n",
      "\n",
      "# how we'll distinguish IG from the suspects\n",
      "mds_markers = ['x']*len(labels)+['o']\n",
      "mds_labels = labels[:]\n",
      "mds_labels.append(labels[-1]+1)\n",
      "\n",
      "# coordinates of 2-D scaled slice vectors\n",
      "xs = new_mds[:,0]\n",
      "ys = new_mds[:,1]\n",
      "\n",
      "# plotting points\n",
      "plt.figure(1, figsize=(10, 10), dpi=200)    \n",
      "for i in range(len(xs)):\n",
      "    plt.plot(xs[i], ys[i], marker = mds_markers[i], color = colors[mds_labels[i]+12])\n",
      "\n",
      "# legend\n",
      "grn_patch = mpatches.Patch(color=colors[len(auth_name)+12], label='\"Grace\"')\n",
      "h_list = [mpatches.Patch(color=colors[i+12], label = auth_frame['AUTH_NAME'][i]) for i in range(len(auth_name))]\n",
      "h_list.append(grn_patch)\n",
      "plt.legend(handles=h_list,loc=0)\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## SMELL TEST\n",
      "\n",
      "import numpy as np\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "from sklearn.manifold import MDS\n",
      "import matplotlib.patches as mpatches\n",
      "\n",
      "# MDS, naturally\n",
      "mds = MDS(n_components=1)\n",
      "\n",
      "for i in range(len(auth_frame['AUTH_NAME'])):\n",
      "    new_array = np.concatenate((bag_array[i*n:(i+1)*n],grc_array), axis=0)\n",
      "    # n here is the number of slices we decided to take earlier\n",
      "    \n",
      "    distances = 1 - cosine_similarity(new_array)\n",
      "\n",
      "    # Scales Cosine Distances into 2-D space\n",
      "    new_mds = mds.fit_transform(new_array)\n",
      "\n",
      "    # how we'll distinguish IG from the suspects\n",
      "    mds_markers = ['x']*(len(new_array)-1)+['o']\n",
      "    colors = ['b']*(len(new_array)-1)+['r']\n",
      "\n",
      "    # coordinates of 2-D scaled slice vectors\n",
      "    xs = new_mds[:,0] # see 'prcomp(my_data)$x' in R\n",
      "    ys = [0]*len(xs)\n",
      "\n",
      "    # plotting points\n",
      "    plt.figure(1, figsize=(10, 1))    \n",
      "    plt.xlim(-75, 75) \n",
      "    for j in range(len(xs)):\n",
      "        # plots suspects' texts\n",
      "        plt.plot(xs[j], ys[j], marker = mds_markers[j], color = colors[j])\n",
      "\n",
      "    # This is inelegant, but I was having trouble making a pretty legend\n",
      "    red_patch = mpatches.Patch(color='r', label='\"Grace\"')\n",
      "    h_list = [mpatches.Patch(color='b', label = auth_frame['AUTH_NAME'][i])]\n",
      "    h_list.append(red_patch)\n",
      "\n",
      "    #plt.legend(handles=h_list,loc=0)\n",
      "    plt.legend(handles=h_list, bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
      "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
      "    \n",
      "    plt.tick_params(axis='y', which='both', left='off', right='off', labelleft='off')\n",
      "    plt.show()\n",
      "    print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Smell Test, self-contained function\n",
      "\n",
      "\n",
      "ig_path = \"/Users/edwinroland/Desktop/Iterating Grace/Iterating Grace.txt\"\n",
      "suspect_path = \"/Users/edwinroland/Desktop/Iterating Grace/suspects/\"\n",
      "\n",
      "# The paths in this script are set up assuming that there is a folder containing\n",
      "# .txt files for only the suspects' texts, and that there is a separate location\n",
      "# for \"Iterating Grace.txt\". The names of suspect's .txt files will be used to\n",
      "# identify them in the pandas dataframe and visualizations.\n",
      "\n",
      "\n",
      "def smell_test(uk_path=ig_path, suspect_path=suspect_path, slices = \"random\", bag_size = None, num_samples = 100):\n",
      "    # slices determines whether tokens are chosen from texts randomly (\"random\") or from\n",
      "    # continuous passages (\"passage\"); bag_size determines how many tokens to retrieve\n",
      "    # for each slice; IG contains 1971 tokens; trying to call more can enter infinite while loop\n",
      "\n",
      "    from nltk.tokenize import word_tokenize\n",
      "    import string\n",
      "    import os\n",
      "    import pandas as pd\n",
      "    import numpy as np\n",
      "    from random import randint\n",
      "    from sklearn.feature_extraction.text import CountVectorizer\n",
      "    from sklearn.metrics.pairwise import cosine_similarity\n",
      "    from sklearn.manifold import MDS\n",
      "    import matplotlib.patches as mpatches\n",
      "    import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "    # pronoun list to remove from text\n",
      "    engl_pron = ['i', 'me','my', 'mine','myself','you', 'your','yours','yourself',\\\n",
      "                'she', 'her', 'hers', 'herself', 'he', 'him', 'his', 'himself',\\\n",
      "                'it', 'its', 'itself', 'we','us','our', 'ours','ourselves',\\\n",
      "                'yourselves', 'they','them','their', 'theirs', 'themselves',\\\n",
      "                'im', 'youre', 'youll','youd', 'shes', 'hes', 'hed', 'theyre', 'theyd']\n",
      "\n",
      "    # takes list of tokens, returns string of randomly selected tokens separated by space\n",
      "    # (sklearn's \"Vectorizer\" likes to take a string as its input)\n",
      "    def random_bag(tokens, bag_size):\n",
      "        auth_bag = []\n",
      "        counter = []\n",
      "        while len(counter)<bag_size:\n",
      "            j = randint(0,len(tokens)-1)\n",
      "            if j not in counter:\n",
      "                auth_bag.append(tokens[j])\n",
      "                counter.append(j)\n",
      "        return \" \".join(auth_bag)\n",
      "\n",
      "    # same as random_bag, but returns continuous passages of text\n",
      "    def passage_bag(tokens, bag_size):\n",
      "        j = randint(0,len(tokens)-bag_size)\n",
      "        auth_bag = tokens[j:j+bag_size]\n",
      "        return \" \".join(auth_bag)\n",
      "\n",
      "\n",
      "    ## IMPORT TEXTS\n",
      "\n",
      "    # text of \"IG\"\n",
      "    with open(uk_path) as f:\n",
      "        uk_string = f.read()\n",
      "\n",
      "    # text of \"IG\" tokenized (quick and dirty)\n",
      "    # Note: the first step is stripping non-ascii characters, bc python 2.X doesn't like unicode\n",
      "    uk_tokens = [x for x in word_tokenize(\"\".join([x for x in uk_string if ord(x)<128]))\\\n",
      "                 if x not in string.punctuation]\n",
      "    \n",
      "    uk_label = \"Unknown\"\n",
      "    \n",
      "    if bag_size == None:\n",
      "        if len(uk_tokens) < 5000:\n",
      "            bag_size = len(uk_tokens)\n",
      "        else:\n",
      "            bag_size = 5000\n",
      "\n",
      "    # import each suspect's text into a dataframe, tokenizes text as well\n",
      "\n",
      "    auth_frame = pd.DataFrame()\n",
      "    auth_name = []\n",
      "    auth_text = []\n",
      "    for x in [x for x in os.listdir(suspect_path) if x[-4:]=='.txt']:\n",
      "        auth_name.append(x[:-4])\n",
      "        with open(suspect_path+x) as f:\n",
      "            auth_text.append(f.read())\n",
      "    auth_frame['AUTH_NAME'] = auth_name # This column is really just the filenames\n",
      "    auth_frame['TEXT'] = auth_text\n",
      "    auth_frame['TOKENS'] = [[x for x in word_tokenize(\"\".join([x for x in auth_text[i] if ord(x)<128]))\\\n",
      "                             if x not in string.punctuation] for i in range(len(auth_text))]\n",
      "\n",
      "\n",
      "    ## DATA SAMPLING\n",
      "\n",
      "    # Slices texts into num_samples bags, produces feature arrays\n",
      "    # takes a while to run on my computer\n",
      "\n",
      "    bag_list = [] # each list item will be a string of randomly chosen words from suspect's writing\n",
      "    labels = [] # keeps track of which string belongs to which author\n",
      "    \n",
      "    if slices == \"random\":\n",
      "        for i in range(len(auth_frame['TOKENS'])):\n",
      "            for j in range(num_samples):\n",
      "                bag_list.append(random_bag(auth_frame['TOKENS'][i],bag_size=bag_size))\n",
      "                labels.append(i)\n",
      "        uk_bag = [random_bag(uk_tokens,bag_size=bag_size)]\n",
      "\n",
      "\n",
      "    \n",
      "    elif slices == \"passage\":\n",
      "        for i in range(len(auth_frame['TOKENS'])):\n",
      "            for j in range(num_samples):\n",
      "                bag_list.append(passage_bag(auth_frame['TOKENS'][i],bag_size=bag_size))\n",
      "                labels.append(i)\n",
      "        uk_bag = [passage_bag(uk_tokens,bag_size=bag_size)]\n",
      "\n",
      "        \n",
      "    # Vectorizer will take the list of strings and return a sparse matrix with the feature set        \n",
      "\n",
      "    # CountVectorizer is where account for Eder's and Hoover's observations\n",
      "    # remove pronouns; rule of thumb # of fts 500; min_df suggested at 0.6-0.8 (for large # suspects)\n",
      "    v = CountVectorizer(stop_words=engl_pron, max_features=500) #min_df=0.6\n",
      "    bag_mtrx = v.fit_transform(bag_list)\n",
      "    grc_mtrx = v.transform(uk_bag)\n",
      "    \n",
      "    # many functions prefer arrays to sparse matrices, and with # of suspects < 10 and n (slices) < 1000,\n",
      "    # we don't have so much data that an array is unwieldy\n",
      "    bag_array = bag_mtrx.toarray()\n",
      "    grc_array = grc_mtrx.toarray()\n",
      "\n",
      "\n",
      "    ## COSINE DISTANCES & VISUALIZATION\n",
      "\n",
      "    mds = MDS(n_components=1)\n",
      "\n",
      "    for i in range(len(auth_frame['AUTH_NAME'])):\n",
      "        new_array = np.concatenate((bag_array[i*num_samples:(i+1)*num_samples],grc_array), axis=0)\n",
      "\n",
      "        distances = 1 - cosine_similarity(new_array)\n",
      "\n",
      "        # Scales Cosine Distances into 2-D space\n",
      "        new_mds = mds.fit_transform(new_array)\n",
      "\n",
      "        # how we'll distinguish IG from the suspects\n",
      "        mds_markers = ['x']*(len(new_array)-1)+['o']\n",
      "        colors = ['b']*(len(new_array)-1)+['r']\n",
      "\n",
      "        # coordinates of 2-D scaled slice vectors\n",
      "        xs = new_mds[:,0] # see 'prcomp(my_data)$x' in R\n",
      "        ys = [0]*len(xs)\n",
      "\n",
      "        # plotting points\n",
      "        plt.figure(1, figsize=(10, 1))    \n",
      "        xlim(-75, 75) \n",
      "        for j in range(len(xs)):\n",
      "            # plots suspects' texts\n",
      "            plt.plot(xs[j], ys[j], marker = mds_markers[j], color = colors[j])\n",
      "\n",
      "        # This is inelegant, but I was having trouble making a pretty legend\n",
      "        red_patch = mpatches.Patch(color='r', label=uk_label)\n",
      "        h_list = [mpatches.Patch(color='b', label = auth_frame['AUTH_NAME'][i])]\n",
      "        h_list.append(red_patch)\n",
      "\n",
      "        #plt.legend(handles=h_list,loc=0)\n",
      "        plt.legend(handles=h_list, bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
      "               ncol=2, mode=\"expand\", borderaxespad=0.)\n",
      "        \n",
      "        plt.tick_params(axis='y', which='both', left='off', right='off', labelleft='off')\n",
      "        plt.show()\n",
      "        print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
